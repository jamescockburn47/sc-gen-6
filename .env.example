# Local LLM provider defaults
LLM_PROVIDER=llama_cpp
LLM_BASE_URL=http://127.0.0.1:8000/v1
LLM_API_KEY=local-llama
LLM_MODEL_NAME=gpt-oss-20b

# Path to your GGUF model for scripts/start_llama_server.bat
LLAMA_MODEL_PATH=C:\models\gpt-oss-20b.Q4_K_M.gguf

# To use LM Studio instead, override:
# LLM_PROVIDER=lmstudio
# LLM_BASE_URL=http://localhost:1234/v1
# LLM_API_KEY=lm-studio


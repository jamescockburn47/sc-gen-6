[
  {
    "label": "GPT-OSS-20B (MXFP4)",
    "model_name": "gpt-oss-20b",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf",
    "vram_gb": 11.3,
    "description": "Fast 20B model for general use"
  },
  {
    "label": "GPT-OSS-120B (MXFP4) - Fast",
    "model_name": "gpt-oss-120b-mxfp4",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/gpt-oss-120b-GGUF/gpt-oss-120b-MXFP4-00001-of-00002.gguf",
    "vram_gb": 59.0,
    "description": "Largest model - fastest inference, recommended for overnight tasks"
  },
  {
    "label": "DeepSeek R1 14B (Reasoning)",
    "model_name": "deepseek-r1:14b",
    "path": "",
    "provider": "ollama",
    "vram_gb": 8.0,
    "description": "Efficient reasoning model, good balance of speed and intelligence"
  },
  {
    "label": "DeepSeek R1 32B (Reasoning)",
    "model_name": "deepseek-r1:32b",
    "path": "",
    "provider": "ollama",
    "vram_gb": 19.0,
    "description": "Powerful reasoning model for complex cases"
  },
  {
    "label": "Qwen 2.5 32B (Instruct)",
    "model_name": "qwen2.5:32b",
    "path": "",
    "provider": "ollama",
    "vram_gb": 19.0,
    "description": "Strong general purpose model"
  },
  {
    "label": "Qwen3-Next 80B (Hybrid MoE)",
    "model_name": "qwen3-80b",
    "path": "",
    "provider": "ollama",
    "vram_gb": 48.0,
    "description": "80B parameter Hybrid Transformer-Mamba MoE model. Requires ~48GB VRAM."
  },
  {
    "label": "Qwen 2.5 72B (Instruct)",
    "model_name": "qwen2.5-72b",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/Qwen2.5-72B-Instruct-GGUF/Qwen2.5-72B-Instruct-Q4_K_M.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 42.0,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q8_0",
    "cache_type_v": "q8_0",
    "description": "High-performance 72B model. Excellent balance of speed and quality. Flash Attention + Q8 KV cache."
  },
  {
    "label": "Mistral Small 3.2 24B",
    "model_name": "mistral-small-24b",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/Mistral-Small-3.2-24B-Instruct-2506-GGUF/Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 16.0,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q4_0",
    "cache_type_v": "q4_0",
    "description": "Very fast, high-quality 24B model. Flash Attention + quantized KV cache."
  },
  {
    "label": "DeepSeek R1 8B (Reasoning)",
    "model_name": "deepseek-r1-8b",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/DeepSeek-R1-0528-Qwen3-8B-GGUF/DeepSeek-R1-0528-Qwen3-8B-Q4_K_M.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 6.0,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q4_0",
    "cache_type_v": "q4_0",
    "description": "Small, efficient reasoning model. Flash Attention enabled."
  },
  {
    "label": "GPT-OSS-120B (MXFP4)",
    "model_name": "gpt-oss-120b",
    "path": "C:/Users/James/.lmstudio/models/lmstudio-community/gpt-oss-120b-GGUF/gpt-oss-120b-MXFP4-00001-of-00002.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 70.0,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q8_0",
    "cache_type_v": "q8_0",
    "description": "Massive 120B MoE model (requires file merge). Flash Attention + Q8 KV cache. ~70GB VRAM."
  },
  {
    "label": "Qwen3-Next 80B (llama.cpp / Fast)",
    "model_name": "qwen3-80b-fast",
    "path": "C:/Users/James/Desktop/SC Gen 6/Qwen_Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 48.0,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q8_0",
    "cache_type_v": "q8_0",
    "description": "80B Hybrid MoE. Flash Attention + Q8 KV cache. High-performance parallel decoding."
  },
  {
    "label": "Mistral Nemo 12B (Fast)",
    "model_name": "mistral-nemo-12b",
    "path": "C:/Users/James/.lmstudio/models/mistralai/Mistral-Nemo-Instruct-2407.Q4_K_M.gguf",
    "executable": "C:/Users/James/Desktop/SC Gen 6/llama-cpp/llama-server.exe",
    "provider": "llama_cpp",
    "vram_gb": 8.5,
    "context": 32768,
    "gpu_layers": 999,
    "parallel": 8,
    "batch": 512,
    "flash_attn": true,
    "cache_type_k": "q4_0",
    "cache_type_v": "q4_0",
    "description": "Ultra-fast 12B model for Fact Lookup. Flash Attention + quantized KV cache for maximum speed. 128k context window."
  }
]